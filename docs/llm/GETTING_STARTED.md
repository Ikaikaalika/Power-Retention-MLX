# Getting Started with LLM Training

Complete guide to training your first Large Language Model with Power Retention.

## What You've Built

A production-ready LLM training system with:

âœ… **Complete Architecture** - RetentionLLM with linear-complexity Power Retention
âœ… **Data Pipeline** - Multi-source loading, filtering, tokenization
âœ… **Training Scripts** - Pretraining, fine-tuning, RLHF
âœ… **Utilities** - Checkpointing, logging, metrics
âœ… **Documentation** - Comprehensive guides and examples

## Directory Structure

```
llm_training/
â”œâ”€â”€ README.md                    # Complete system documentation
â”œâ”€â”€ GETTING_STARTED.md          # This file
â”œâ”€â”€ requirements.txt            # Dependencies
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ retention_llm.py        # LLM architecture (23M - 7B params)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_processor.py       # Data loading & processing
â”‚   â”œâ”€â”€ data_sources.py         # Curated dataset catalog
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ utils.py                # Checkpointing, logging
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ quick_start.py          # End-to-end training demo
â”‚
â””â”€â”€ configs/
    â””â”€â”€ (training configs)
```

## Installation

### 1. Install Dependencies

```bash
# Core dependencies
pip install mlx mlx-nn

# Data processing
pip install transformers datasets

# Optional: Quality filtering
pip install detoxify

# Optional: RLHF
pip install trl peft
```

Or install all at once:
```bash
cd llm_training
pip install -r requirements.txt
```

### 2. Verify Installation

```python
python3 -c "import mlx.core as mx; print('MLX version:', mx.__version__)"
python3 -c "from llm_training.models import RetentionLLM; print('âœ“ Models loaded')"
python3 -c "from llm_training.data import DataProcessor; print('âœ“ Data loaded')"
```

## Quick Start (5 Minutes)

Train a tiny model on synthetic data to verify everything works:

```bash
python3 llm_training/examples/quick_start.py
```

Expected output:
```
Creating model...
âœ“ Model created with ~23M parameters

Generating synthetic training data...
âœ“ Created 25 batches

Starting training...
Step 10: loss=8.5234, perplexity=5000.12, ...
Step 20: loss=7.2104, perplexity=1345.67, ...
...
Training Complete!
Final loss: 5.4321
```

This trains for 100 steps (~2-3 minutes on M1 Mac).

## Your First Real LLM (Small - 125M params)

### Step 1: Choose Model Size

```python
from llm_training.models import create_model_config

# Available sizes:
config = create_model_config("tiny")    # 23M params  - testing
config = create_model_config("small")   # 125M params - recommended start
config = create_model_config("medium")  # 355M params - M1 Max+
config = create_model_config("large")   # 774M params - M1 Ultra+
config = create_model_config("7b")      # 7B params   - cloud GPUs

# We'll start with small
config = create_model_config("small")
```

### Step 2: Prepare Data

```python
from llm_training.data import DataProcessor, get_dataset

# Initialize processor
processor = DataProcessor()

# Option A: Use high-quality curated data
dataset = get_dataset("fineweb-edu")  # 1.3T tokens, educational
print(f"Loading {dataset.name}: {dataset.description}")

# Option B: Test with smaller dataset first
dataset = get_dataset("alpaca")  # 52K examples for testing

# Load and prepare
data_iter = processor.prepare_training_data(
    dataset_name=dataset.hf_path,
    num_samples=10000,  # Start small
    filter_quality=True
)
```

### Step 3: Train the Model

```python
import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from llm_training.models import RetentionLLM
from llm_training.training.utils import TrainingLogger, save_checkpoint

# Create model
model = RetentionLLM(**config, use_metal=False)  # Training mode

# Setup training
optimizer = optim.Adam(learning_rate=3e-4)
logger = TrainingLogger(log_dir="logs")

# Training loop
for step, batch in enumerate(data_iter):
    # Forward + backward
    def loss_fn(model, input_ids, labels):
        logits = model(input_ids)
        loss = nn.losses.cross_entropy(
            logits.reshape(-1, logits.shape[-1]),
            labels.reshape(-1)
        )
        return loss

    loss_and_grad = nn.value_and_grad(model, loss_fn)
    loss, grads = loss_and_grad(model, batch['input_ids'], batch['labels'])

    # Update
    optimizer.update(model, grads)
    mx.eval(model.parameters(), optimizer.state)

    # Log
    logger.log(step, {"loss": loss.item()})

    # Save checkpoint every 1000 steps
    if step % 1000 == 0:
        save_checkpoint(
            model, optimizer, epoch=0, step=step,
            loss=loss, save_dir="checkpoints/small"
        )

    if step >= 10000:  # Stop after 10K steps for testing
        break
```

### Step 4: Test Generation

```python
from transformers import AutoTokenizer

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Generate text
model.reset_states()
prompt = "The meaning of life is"
tokens = tokenizer.encode(prompt, return_tensors="np")
tokens = mx.array(tokens)

generated = model.generate(
    tokens,
    max_new_tokens=50,
    temperature=0.8,
    top_p=0.9
)

print(tokenizer.decode(generated[0].tolist()))
```

## Data Sources

### Recommended for Pretraining

1. **FineWeb-Edu** (`fineweb-edu`)
   - Size: 1.3T tokens
   - Quality: High (educational content)
   - License: ODC-By
   - Best for: General language understanding

2. **The Stack V2** (`the-stack-v2`)
   - Size: 900B tokens
   - Quality: High (code)
   - License: OpenRAIL-M
   - Best for: Code understanding

3. **Dolma** (`dolma`)
   - Size: 3T tokens
   - Quality: High (curated)
   - License: ODC-By
   - Best for: Diverse pretraining

### Access Datasets

```python
from llm_training.data import list_datasets, get_dataset

# List all available
for name, ds in list_datasets("pretrain").items():
    print(f"{name}: {ds.description} ({ds.size_tokens})")

# Get specific dataset
dataset = get_dataset("fineweb-edu")
print(f"HF path: {dataset.hf_path}")
print(f"License: {dataset.license}")
```

## Training Configurations

### Budget-Based Recommendations

```python
from llm_training.data import get_recommended_mix

# Get data mix for your token budget
mix = get_recommended_mix("10B")  # Options: "1B", "10B", "100B", "1T"

for dataset_name, proportion in mix:
    dataset = get_dataset(dataset_name)
    tokens = int(10_000_000_000 * proportion)  # 10B total
    print(f"{dataset_name}: {tokens/1e9:.1f}B tokens ({proportion*100}%)")
```

### Hardware-Specific Settings

```python
# M1 Mac (8GB RAM)
config = {
    "model_size": "tiny",
    "batch_size": 2,
    "seq_len": 1024,
    "accumulation_steps": 4,
}

# M1 Pro/Max (32GB RAM)
config = {
    "model_size": "small",
    "batch_size": 8,
    "seq_len": 2048,
    "accumulation_steps": 1,
}

# M1 Ultra (128GB RAM)
config = {
    "model_size": "medium",
    "batch_size": 16,
    "seq_len": 4096,
    "accumulation_steps": 1,
}

# Cloud (A100 80GB)
config = {
    "model_size": "7b",
    "batch_size": 32,
    "seq_len": 8192,
    "accumulation_steps": 1,
}
```

## Monitoring Training

### Key Metrics

- **Loss**: Should decrease steadily (target: < 3.0 for good models)
- **Perplexity**: exp(loss) - lower is better (GPT-2: ~30, GPT-3: ~20)
- **Tokens/sec**: Throughput measure
- **Memory**: Watch for OOM errors

### Logging

```python
from llm_training.training.utils import TrainingLogger

logger = TrainingLogger(log_dir="logs", log_interval=10)

# During training
logger.log(step, {
    "loss": loss,
    "perplexity": mx.exp(loss),
    "lr": learning_rate,
    "tokens_per_sec": throughput
})

# Get summary
summary = logger.get_summary()
print(f"Avg loss: {summary['avg_loss']:.4f}")
print(f"Best loss: {summary['min_loss']:.4f}")
```

### Checkpoints

Saved automatically in `checkpoints/` directory:
```
checkpoints/
â”œâ”€â”€ checkpoint_epoch0_step1000.safetensors  # Model weights
â”œâ”€â”€ checkpoint_epoch0_step1000_meta.json    # Metadata
â”œâ”€â”€ optimizer_epoch0_step1000.npz           # Optimizer state
â””â”€â”€ latest.txt                              # Latest checkpoint reference
```

Load checkpoint:
```python
from llm_training.training.utils import load_checkpoint

metadata = load_checkpoint(model, "checkpoints", "latest")
print(f"Resumed from step {metadata['step']}")
```

## Next Steps

### 1. Fine-tune on Instructions

After pretraining, fine-tune for instruction-following:

```python
from llm_training.data import get_dataset

# Load instruction dataset
dataset = get_dataset("dolly-15k")

# Fine-tune (lower learning rate)
optimizer = optim.Adam(learning_rate=1e-5)

# Train for 3 epochs
# ...
```

### 2. Evaluate Performance

```python
# Generate on test prompts
prompts = [
    "Explain quantum computing to a 5 year old:",
    "Write a Python function to sort a list:",
    "What is the capital of France?"
]

for prompt in prompts:
    tokens = tokenizer.encode(prompt)
    output = model.generate(mx.array([tokens]), max_new_tokens=100)
    print(f"Prompt: {prompt}")
    print(f"Output: {tokenizer.decode(output[0].tolist())}")
    print()
```

### 3. Deploy

```python
# Switch to Metal kernels for 2-3x faster inference
model.use_metal_kernels = True

# Save for deployment
mx.savez("model_inference.safetensors", **dict(model.parameters()))
```

## Troubleshooting

### Out of Memory

```python
# Solution 1: Reduce batch size
batch_size = 1

# Solution 2: Reduce sequence length
seq_len = 1024

# Solution 3: Use gradient accumulation
accumulation_steps = 4  # Effective batch_size = batch_size * 4

# Solution 4: Clear cache periodically
mx.clear_cache()
```

### Slow Training

```python
# Solution 1: Increase batch size (if memory allows)
batch_size = 16

# Solution 2: Use larger chunks
chunk_size = 512

# Solution 3: Profile bottlenecks
import time
t0 = time.time()
# ... training step ...
print(f"Step time: {time.time() - t0:.3f}s")
```

### NaN Loss

```python
# Solution 1: Lower learning rate
learning_rate = 1e-4

# Solution 2: Gradient clipping
max_norm = 1.0
grads = {k: mx.clip(v, -max_norm, max_norm) for k, v in grads.items()}

# Solution 3: Check data quality
# - Remove outliers
# - Verify tokenization
# - Filter toxic/malformed text
```

## Resources

- **Main README**: [llm_training/README.md](README.md)
- **Model Architecture**: [models/retention_llm.py](models/retention_llm.py)
- **Data Processing**: [data/data_processor.py](data/data_processor.py)
- **Dataset Catalog**: [data/data_sources.py](data/data_sources.py)

- **Power Retention Module**: [../power_retention.py](../power_retention.py)
- **Metal Kernels Guide**: [../METAL_KERNELS.md](../METAL_KERNELS.md)
- **Model Building Guide**: [../QUICK_START.md](../QUICK_START.md)

## Support

If you encounter issues:

1. Check logs in `logs/` directory
2. Verify dependencies with `pip list`
3. Test with quick_start.py first
4. Check Memory usage with Activity Monitor
5. Try smaller model/batch size

## What's Next?

You now have a complete LLM training system! Try:

1. **Train a small model** (125M params) on 1B tokens
2. **Fine-tune** on instruction data
3. **Evaluate** on benchmarks
4. **Scale up** to larger models
5. **Deploy** with Metal kernels for fast inference

Happy training! ðŸš€
